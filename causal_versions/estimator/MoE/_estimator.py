from typing import Any

import numpy as np
from rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn
from scipy.special import logsumexp

from causal_versions.estimator.linear import LinearModel
from causal_versions.estimator.mnlogit._estimator import MultinomialLogisticRegression


class MoE:
    """
    Mixture-of-experts via EM algorithm
    """

    def __init__(
        self,
        n_components=2,
        init_params="kmeans",
        max_iter=100,
        tol=1e-5,
        random_state=None,
    ):
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
        self.n_components = n_components
        self.init_params = init_params
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state

        # EM ã§æ±‚ã¾ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹å±æ€§ï¼ˆfit ã§è¨­å®šï¼‰
        # initialization :
        self.gate = MultinomialLogisticRegression()  # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æ··åˆä¿‚æ•°é–¢æ•° Ï€_v (X_i, W_i), mixture of experts ã® gate é–¢æ•°
        self.density: dict[Any, LinearModel] = {}  # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚»ãƒŸãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯å¯†åº¦ f_v(Y_i | X_i) ã‚’ä¿æŒã™ã‚‹è¾æ›¸
        for v in range(self.n_components):
            self.density[v] = LinearModel()
        self.posterior: np.ndarray

        self.log_likelihood_ = []  # å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å¯¾æ•°å°¤åº¦å±¥æ­´
        self.n_iter_ = 0  # å®Ÿéš›ã«è¡Œã£ãŸã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°
        self.converged_ = False  # åæŸãƒ•ãƒ©ã‚°

    def fit(self, Y, X):
        """
        X: å½¢çŠ¶ (n_samples, n_features) ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—
        y: ä½¿ã‚ãªã„ï¼ˆæ··åˆãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ï¼‰ã®ã§ None
        """
        # ç‰¹å¾´é‡åã®ä¿å­˜
        if getattr(X, "columns", None) is not None:
            self.x_feature_names_in_ = X.columns.tolist()

        if getattr(Y, "name", None) is not None:
            self.y_feature_names_in_ = Y.name

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨å½¢çŠ¶å–å¾—
        X = np.asarray(X)
        Y = np.asarray(Y).reshape(-1, 1)
        self.n_samples, self.n_features_x = X.shape

        # åˆæœŸåŒ–å‡¦ç†
        # random initialization or k-means
        # k_means_model = KMeans(
        #     n_clusters=self.n_components,
        #     n_init=20,  # è¤‡æ•°åˆæœŸåŒ–ã§ã‚ˆã‚Šå®‰å®š
        #     random_state=self.random_state,
        # )
        # gmm = GaussianMixture(n_components=self.n_components, init_params="k-means++", random_state=self.random_state, tol=1e-5)

        # init_data = np.concatenate([X], axis=1)
        # init_data = StandardScaler().fit_transform(init_data)

        # gmm.fit(init_data)
        rng = np.random.default_rng(self.random_state)
        alpha = np.ones(self.n_components)
        # shape: (n_samples, K)
        self.posterior = rng.dirichlet(alpha, size=self.n_samples)

        # self.posterior = np.zeros(
        #     (self.n_samples, self.n_components)
        # )  # å½¢: (n_samples, n_components)  # å„ã‚µãƒ³ãƒ—ãƒ« i ã«å¯¾ã™ã‚‹å„ã‚¯ãƒ©ã‚¹ã‚¿ v ã® posterior ç¢ºç‡ p_iv (n_samples, n_components)

        # labels ç•ªå·ã‚’ç”¨ã„ã¦ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã®åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        # self.posterior[np.arange(self.n_samples), labels] = 1.0  # åˆæœŸè²¬ä»»åº¦ã‚’è¨­å®š

        # å¯¾æ•°å°¤åº¦åˆæœŸåŒ–
        prev_log_likelihood = -np.inf

        # Rich ã® Progress ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        progress = Progress(
            TextColumn("[bold blue]EM Iteration[/]"),
            BarColumn(bar_width=None),
            TextColumn("{task.completed}/{task.total} iter"),
            TimeElapsedColumn(),
            TextColumn(" | log_likelihood = "),
            TextColumn("[green]{task.fields[ll]:.6f}[/]"),
            ## ã“ã“ã§æ›´æ–°é »åº¦ã‚’æŠ‘åˆ¶
            refresh_per_second=1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 10 å›ï¼ç§’ã€‚1 ã«ã™ã‚Œã°ï¼‘ç§’ã«ï¼‘å›æ›´æ–°
            transient=False,  # æœ€å¾Œã®æç”»ã‚’æ®‹ã™ã‹ã©ã†ã‹
            redirect_stdout=False,  # stdout ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’æŠ‘åˆ¶
            redirect_stderr=False,
            disable=True,
        )
        with progress:
            task = progress.add_task("", total=self.max_iter, ll=prev_log_likelihood)

            for iteration in range(self.max_iter):
                # print(f"EM Iteration {iteration + 1}/{self.max_iter}")
                self.n_iter_ = iteration + 1

                # =============================
                # 1) M_step: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
                # =============================
                #   (a) æ··åˆä¿‚æ•° Ï€_v (X_i, W_i) ã‚’æ›´æ–°
                #   (b) ã‚¢ã‚¦ãƒˆã‚«ãƒ ãƒ¢ãƒ‡ãƒ«ã®æ¨å®š
                self._m_step(Y, X)

                # =============================
                # 2) E_step: æ½œåœ¨å¤‰æ•°æœŸå¾…å€¤ã®è¨ˆç®—
                # =============================
                # å„ã‚µãƒ³ãƒ—ãƒ« i, å„ã‚¯ãƒ©ã‚¹ã‚¿ v ã«å¯¾ã™ã‚‹ã€Œè²¬ä»»åº¦ã€ï¼ˆposteriorï¼‰ã‚’è¨ˆç®—
                self._e_step(Y, X)

                # =============================
                # 3) åæŸåˆ¤å®š: å¯¾æ•°å°¤åº¦ã‚’è¨ˆç®—ã—ã€åæŸæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                # =============================
                curr_log_likelihood = self.compute_log_likelihood(Y, X)
                self.log_likelihood_.append(curr_log_likelihood)

                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°ã—ã¦å†æç”»
                progress.update(task, advance=1, ll=curr_log_likelihood)

                # åæŸåˆ¤å®šé–¢æ•°ã‚’å‘¼ã³å‡ºã—
                if self.check_convergence(curr_log_likelihood, prev_log_likelihood, tol=self.tol):
                    self.converged_ = True
                    print("Converged at iteration", iteration + 1)
                    break

                prev_log_likelihood = curr_log_likelihood

            # end of for loop ---

        # æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸¦ã³æ›¿ãˆ
        self._reorder_components()

        return self

    def _e_step(self, Y, X) -> None:
        """
        E ã‚¹ãƒ†ãƒƒãƒ—ï¼šç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€å„ã‚µãƒ³ãƒ—ãƒ«ã¨å„ã‚¯ãƒ©ã‚¹ã‚¿ã®
        posterior posterior r_ij ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        æˆ»ã‚Šå€¤: r (å½¢çŠ¶ n_samples x n_components ã®è¡Œåˆ—)
        å„ã‚µãƒ³ãƒ—ãƒ« i, å„ã‚¯ãƒ©ã‚¹ã‚¿ v ã«å¯¾ã™ã‚‹ã€Œè²¬ä»»åº¦ã€ï¼ˆposteriorï¼‰ã‚’è¨ˆç®—

        Input:
            Y
            X
            W
        Output:
        posterior : ndarray of shape (n_samples, n_components)
            å„ã‚µãƒ³ãƒ—ãƒ« i ã«å¯¾ã™ã‚‹å„ã‚¯ãƒ©ã‚¹ã‚¿ v ã® posterior ç¢ºç‡ r_ij
            å½¢: r_ij = p_iv = Ï€_v(X_i, W_i) * ğ’©_h [f_v(Y_i | X_i)] / Î£_{k} Ï€_k * ğ’©_h [f_k(Y_i | X_i)]
            ã“ã“ã§ã€ğ’©_h ã¯ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã®çµæœã‚’è¡¨ã™ã€‚


        """
        # 1) log Ï€_v
        log_pi = np.log(np.clip(self.gate.predict_proba(X), 1e-300, 1.0))  # shape (n,m)

        # 2) log f_v
        log_f_cols = []
        for v in range(self.n_components):
            log_f = self.density[v].log_conditional_density(X, Y)
            log_f_cols.append(log_f)
        log_f = np.column_stack(log_f_cols)  # shape (n,m)

        # 3) log posterior  up to additive const
        log_num = log_pi + log_f  # (n,m)
        log_den = logsumexp(log_num, axis=1, keepdims=True)  # (n,1)

        self.posterior = np.exp(log_num - log_den)  # normalised r_iv

    def _m_step(self, Y, X) -> None:
        """
        M ã‚¹ãƒ†ãƒƒãƒ—ï¼šE ã‚¹ãƒ†ãƒƒãƒ—ã§å¾—ãŸ posterior ã‚’ç”¨ã„ã¦ã€
        æ··åˆä¿‚æ•°ã¨å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€‚
        æˆ»ã‚Šå€¤: æ›´æ–°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # 1) æ··åˆä¿‚æ•° Ï€_v (X_i, W_i) ã®æ›´æ–°
        self.gate.fit(X, y_soft=self.posterior)

        # 2) å¯†åº¦ f_v(Y_i | X_i) ã®æ›´æ–°
        for v in range(self.n_components):
            self.density[v].fit(X, Y, sample_weight=self.posterior[:, v])

    def compute_log_likelihood(self, Y, X) -> float:
        """
        ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚µãƒ³ãƒ—ãƒ«å…¨ä½“ã®å¯¾æ•°å°¤åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        Parameters
        ----------
        Y : array-like of shape (n_samples,)
            ç›®çš„å¤‰æ•°ã®è¦³æ¸¬å€¤
        X : array-like of shape (n_samples, n_features_x)
            èª¬æ˜å¤‰æ•°ã®ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—
        W : array-like of shape (n_samples, n_features_w)
            è¿½åŠ ã®èª¬æ˜å¤‰æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

        Returns
        -------
        log_likelihood : float
            å¯¾æ•°å°¤åº¦ã®å€¤
        """
        # Ï€_v(X,W)
        pi = self.gate.predict_proba(X)  # (n, m)
        log_pi = np.log(np.clip(pi, 1e-300, 1.0))  # underflow é˜²æ­¢

        # 2) log f_v(y|x)  å„ã‚¯ãƒ©ã‚¹ã‚¿åˆ—ã‚’æ§‹ç¯‰
        log_f_cols = []
        for v in range(self.n_components):
            log_f = self.density[v].log_conditional_density(X, Y)
            log_f_cols.append(log_f)
        log_f = np.column_stack(log_f_cols)

        # 3) log-sum-exp over components, then sum over samples
        loglik = logsumexp(log_pi + log_f, axis=1).sum()
        return float(loglik)

    def check_convergence(self, curr, prev, tol):
        """
        å¯¾æ•°å°¤åº¦ã®å¤‰åŒ–ãŒ tol ä»¥ä¸‹ã§ã‚ã‚Œã°åæŸã¨ã¿ãªã™ã€‚
        prev ãŒ -inf ã®å ´åˆã¯åæŸã—ãªã„ã€‚
        """
        if prev == -np.inf:
            return False
        return np.abs(curr - prev) / (np.abs(prev) + 1e-12) < tol

    def _component_parameter_vector(self, index: int) -> np.ndarray:
        """
        å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã—ã€ä¸¦ã¹æ›¿ãˆã®ã‚­ãƒ¼ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã€‚
        LinearModel ã®å ´åˆã¯ä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ã€KDE ç³»ã§ã¯ã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é€£çµã™ã‚‹ã€‚
        """
        model = self.density[index]

        # LinearModel ç­‰: coef_ ã‚„ sigma_ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«å®‰å…¨ã«å–å¾—ã—ã¦é€£çµã™ã‚‹
        if getattr(model, "coef_", None) is not None:
            coef = np.asarray(getattr(model, "coef_", [])).ravel()
            sigma = np.asarray(getattr(model, "sigma_", [])).ravel()

            pieces: list[np.ndarray] = []
            if coef.size:
                pieces.append(coef)
            if sigma.size:
                pieces.append(sigma)

            if pieces:
                return np.concatenate(pieces)
            # coef_ ã‚‚ sigma_ ã‚‚ç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            raise ValueError(f"Component model at index {index} has no coef_ or sigma_ parameters.")

        # KDE ç³»ãªã©: kernel_params ã‚’è¾æ›¸çš„ã«é€£çµ
        if getattr(model, "kernel_params", None) is not None:
            params = model.kernel_params
            pieces: list[np.ndarray] = []
            for key in sorted(params):
                value = params[key]
                if value is None:
                    pieces.append(np.array([np.nan]))
                else:
                    pieces.append(np.atleast_1d(np.asarray(value)).ravel())
            if pieces:
                return np.concatenate(pieces)
            return np.array([0.0])

        raise AttributeError(f"Component model at index {index} does not expose sortable parameters.")

    def _reorder_components(self) -> None:
        """
        ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¾æ›¸é †ã«ä¸¦ã³æ›¿ãˆã€posterior ã¨ã‚²ãƒ¼ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ•´åˆã•ã›ã‚‹ã€‚
        """
        if self.n_components <= 1:
            return

        sort_keys = [tuple(self._component_parameter_vector(v)) for v in range(self.n_components)]
        permutation = np.array(
            sorted(range(self.n_components), key=lambda idx: sort_keys[idx]),
            dtype=int,
        )

        if np.array_equal(permutation, np.arange(self.n_components)):
            # æ—¢ã«è¾æ›¸é †
            self.component_permutation_ = permutation
            return

        # posterior ã®åˆ—ã‚’ä¸¦ã³æ›¿ãˆ
        if hasattr(self, "posterior"):
            self.posterior = self.posterior[:, permutation]

        # density ã‚’æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å†å‰²ã‚Šå½“ã¦
        reordered_density: dict[int, Any] = {}
        for new_idx, old_idx in enumerate(permutation):
            reordered_density[new_idx] = self.density[old_idx]
        self.density = reordered_density

        # ã‚²ãƒ¼ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å›ºå®š
        self.gate.relabel(permutation, only_order=True)

        self.component_permutation_ = permutation
